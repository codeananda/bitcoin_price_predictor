{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The PyTorch Tutorials\n",
    "\n",
    "Notes following along with the [beginner NLP PyTorch tutorials](https://github.com/pytorch/tutorials/tree/master/beginner_source/nlp).\n",
    "\n",
    "## Introduction to PyTorch\n",
    "\n",
    "[Article 1](https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T15:12:29.348171Z",
     "start_time": "2021-03-02T15:12:28.750876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12b91fd10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tensors\n",
    "\n",
    "Just wrap a list in `torch.tensor()` and you are good. \n",
    "\n",
    "Remember that the shape of a tensor is (2, 3, 4) means there are 2 tensors x 3 rows x 4 columns. \n",
    "\n",
    "You can slice tensors as expected.\n",
    "\n",
    "To get Python numbers from them call `.item()` on the particular slice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T15:12:29.356152Z",
     "start_time": "2021-03-02T15:12:29.350390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([[[1., 2.],\n",
      "         [3., 4.]],\n",
      "\n",
      "        [[5., 6.],\n",
      "         [7., 8.]]])\n"
     ]
    }
   ],
   "source": [
    "V_data = [1.0, 2.0, 3.0]\n",
    "V = torch.tensor(V_data)\n",
    "print(V)\n",
    "\n",
    "T_data = [[[1.0, 2.0], [3.0, 4.0]], [[5.0, 6.0], [7.0, 8.0]]]\n",
    "T = torch.tensor(T_data)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T15:12:29.362455Z",
     "start_time": "2021-03-02T15:12:29.357849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "1.0\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "print(V[0])\n",
    "print(V[0].item())\n",
    "\n",
    "print(T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T15:12:29.366362Z",
     "start_time": "2021-03-02T15:12:29.364365Z"
    }
   },
   "outputs": [],
   "source": [
    "# Raises ValueError: only one element tensors can be\n",
    "# converted to Python scalars\n",
    "# print(T[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads of ways to [combine tensors together](https://pytorch.org/docs/torch.html). \n",
    "\n",
    "One example is concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T15:12:29.369569Z",
     "start_time": "2021-03-02T15:12:29.367880Z"
    }
   },
   "outputs": [],
   "source": [
    "# By default it concatenates along the first axis (rows)\n",
    "# Note that this is the same as adding the"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that concatenation can only happen if one of the shapes of either the rows or columns is the same. Apart from square tenors, you can only concatenate along the rows that have different shapes.\n",
    "\n",
    "To add more columns, we must have the same number of rows. To add more rows, we must have the same number of columns. Light bulb moment.\n",
    "\n",
    "Vectors with shape (3, 4) and (2, 4) can be concatenated along rows to give (5, 4) shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T15:12:29.375716Z",
     "start_time": "2021-03-02T15:12:29.370843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "# Default is to cat along rows i.e. axis=0\n",
    "# as if we are looking at a shape vector, s, to access the row\n",
    "# shape we do s[0] to access column shape we do s[1]\n",
    "# all makes so much sense now!\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1.shape)\n",
    "\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T15:12:29.383261Z",
     "start_time": "2021-03-02T15:12:29.380248Z"
    }
   },
   "outputs": [],
   "source": [
    "# If tensors not compatible, PyTorch will complain\n",
    "# torch.cat([x_1, x_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping\n",
    "\n",
    "Use the `.view()` method to reshape a tensor. Note that `.view()` never copies memory. Ever. Reshape and resize either copy memory or recreate the tensor entirely. I imagine this is vital since we want to just move the shape around a bit but still remember everything in the past like gradients and such.\n",
    "\n",
    "Note: `.stride()` method returns the shape of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T15:12:29.391188Z",
     "start_time": "2021-03-02T15:12:29.386215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 4, 1)\n",
      "(12, 1)\n",
      "(12, 1)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x.stride())\n",
    "print(x.view(2, 12).stride())\n",
    "print(x.view(2, -1).stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graphs\n",
    "\n",
    "Obvs we will be using computation graphs for our NNs. \n",
    "\n",
    "By default user created tensors do not remember the gradients. We can modify this though. All tensor factory methods hav ea `requires_grad` flag. \n",
    "\n",
    "Now the tensor remembers how it was created. This is vital if we are to compute derivatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T15:12:29.398239Z",
     "start_time": "2021-03-02T15:12:29.393147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 7., 9.], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x12c0e9150>\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
    "z = x + y\n",
    "print(z)\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you keep following `z.grad_fn` you will return to x and y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T15:12:29.403759Z",
     "start_time": "2021-03-02T15:12:29.400042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(21., grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x12c0e9190>\n"
     ]
    }
   ],
   "source": [
    "s = z.sum()\n",
    "print(s)\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are always calculating partial derivatives (since obvs taking full derivatives for multivariate functions is super hard. So we take, for example, the derivative of s wrt x_0 (i.e. the first element of x).\n",
    "\n",
    "Well, to get s we did\n",
    "\n",
    "```python\n",
    "s = x_0 + y_0 + x_1 + y_1 + x_2 + y_2\n",
    "```\n",
    "\n",
    "And ds/dx_0 = 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T15:12:29.409512Z",
     "start_time": "2021-03-02T15:12:29.405516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Call this on any variable to run backprop starting from this\n",
    "# variable (all the way to the end I think)\n",
    "s.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run this multiple times, it will increment the gradient. PyTorch accuumulates the gradient into the `.grad` property since for many models this is convenient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T15:12:29.414616Z",
     "start_time": "2021-03-02T15:12:29.411122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2)\n",
    "y = torch.randn(2, 2)\n",
    "print(x.requires_grad, y.requires_grad)\n",
    "z = x + y\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T15:12:29.419164Z",
     "start_time": "2021-03-02T15:12:29.416063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x12c0f9690>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# This happens in-place but also returns the object\n",
    "# In the tutorial he reassigns the output to x which is\n",
    "# unnecessary\n",
    "x.requires_grad_()\n",
    "y.requires_grad_()\n",
    "z = x + y\n",
    "print(z.grad_fn)\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop tracing history on Tensors with `requires_grad=True` with the context manager `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T15:14:17.009597Z",
     "start_time": "2021-03-02T15:14:17.002385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "[Article](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html).\n",
    "\n",
    "Idea super relevant to time series forcasting:\n",
    "\n",
    "Recall that in an n-gram language model, given a sequence of words _w_, we want to compute \n",
    "\n",
    "```python\n",
    "P(w_i | w_(i-1), w_(i-2), ..., w(i_n+1))\n",
    "```\n",
    "\n",
    "Where `w_i` is the ith word of the sequence. In other words, we are predicting the ith word using _all of the previous words we have seen_.\n",
    "\n",
    "I understand pretty much everything apart from the line in `class NGramLanguageModeler(nn.Module):` class definition saying\n",
    "\n",
    "```python\n",
    "self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "```\n",
    "\n",
    "No idea where he got `context_size * embedding_dim` from. I think it's because the context is 2 words long and so the linear transformation is obvs going to include the context but also the embeddings. \n",
    "\n",
    "OOOOOH SHIT. Ok so the model does not actually do anything with regard to the targets. It doesn't care about the targets. It is going to use the 2 previous words (the context) to predict the third word. So obvs the input is the context words and the output (that the model will actually work with) are probabilities of what the next word will be. We then compare this output to the actual output we want (i.e. the target) and then update the weights of the model accordingly to be more aligned with the output we want. \n",
    "\n",
    "Then the output for a user may be a particular word. But an NN is always going to output softmax probabilities and show you the range of possible values it thinks it could be. Clever. Very clever. \n",
    "\n",
    "Wou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
